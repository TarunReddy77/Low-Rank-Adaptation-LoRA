# LoRA: Low‑Rank Adaptation of Large Language Models

[![Live](https://img.shields.io/badge/Live-GitHub%20Pages-1461DE)](https://tarunreddy77.github.io/Low-Rank-Adaptation-LoRA/) ![Last commit](https://img.shields.io/github/last-commit/TarunReddy77/Low-Rank-Adaptation-LoRA) ![License](https://img.shields.io/badge/License-MIT-green)

An accessible, visual explainer of the paper “[LoRA: Low‑Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)”. It covers the method, results, comparisons to related work, author bios, impact, and follow‑on research.

![Preview](images/architecture.png)

## What’s inside

- **Methodology**: How LoRA injects low‑rank adapters and why it works
- **Results**: Performance vs. full fine‑tuning, rank sensitivity
- **Literature review**: FT, BitFit, Prefix/Adapter tuning comparisons
- **Author biographies**: Short profiles of the paper’s authors
- **Impact & Applications**: Social/industry implications, deployment
- **Follow‑on research**: QLoRA, QALoRA, GLoRA and more

## Live site

Deployed on GitHub Pages: `https://tarunreddy77.github.io/Low-Rank-Adaptation-LoRA/`

## Quick start

1. Clone the repo
2. Open `index.html` in your browser

## Tech stack

- HTML, CSS, Bootstrap, MathJax
- Responsive layout, accessible alt text, social cards (OpenGraph/Twitter)

## Repository structure

```
.
├── index.html
├── style.css
├── images/
└── README.md
```

## Credits

- Paper: “[LoRA: Low‑Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)”
- Images/figures belong to their respective authors; used for educational purposes

## License

This project is licensed under the MIT License. See `LICENSE`.